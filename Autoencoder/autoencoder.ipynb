{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vtk import *\n",
    "from vtk.util import numpy_support\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_PSNR(arrgt,arr_recon):\n",
    "    try:\n",
    "        diff = arrgt - arr_recon\n",
    "        sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n",
    "        if(np.mean(diff**2) == 0):\n",
    "            raise ZeroDivisionError(\"dividing by zero, cannot calculate psnr\")\n",
    "        snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n",
    "        return snr\n",
    "    except ZeroDivisionError as err:\n",
    "        return str(err)\n",
    "\n",
    "\n",
    "\n",
    "def read_vti(filename):\n",
    "    reader = vtkXMLImageDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "\n",
    "\n",
    "def writeVti(data, filename):\n",
    "    writer = vtkXMLImageDataWriter()\n",
    "    writer.SetFileName(filename)\n",
    "    writer.SetInputData(data)\n",
    "    writer.Write()\n",
    "\n",
    "\n",
    "def createVtkImageData(origin, dimensions, spacing):\n",
    "    localDataset = vtkImageData()\n",
    "    localDataset.SetOrigin(origin)\n",
    "    localDataset.SetDimensions(dimensions)\n",
    "    localDataset.SetSpacing(spacing)\n",
    "    return localDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 50)\n",
      "float32\n",
      "Corrected input tensor shape: torch.Size([1, 1, 250, 250, 50])\n"
     ]
    }
   ],
   "source": [
    "directory_path = '/Users/manasvijain/Desktop/Autoencoder/matrices' \n",
    "files = [f\"Matrix_{i}.txt\" for i in range(50)]\n",
    "matrices = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory_path, file) \n",
    "    matrix = np.loadtxt(file_path)  \n",
    "    if matrix.shape != (250, 250):\n",
    "        raise ValueError(f\"Expected matrix size 250x250, but got {matrix.shape} in file {file}\")\n",
    "    matrices.append(matrix)  \n",
    "\n",
    "data = np.stack(matrices, axis=2)\n",
    "data = data.astype(np.float32)\n",
    "# data = (data - data.min()) / (data.max() - data.min())\n",
    "print(data.shape)  # Should be (250, 250, 50)\n",
    "print(data.dtype)\n",
    "four_d_tensor = torch.from_numpy(data).float() \n",
    "input_tensor = four_d_tensor.unsqueeze(0).unsqueeze(0)\n",
    "print(\"Corrected input tensor shape:\", input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[31m2024-12-13 18:16:01.585 (71346.789s) [           9963E]       vtkXMLParser.cxx:368    ERR| vtkXMLDataParser (0x1112e3020): Error parsing XML in stream at line 1, column 0, byte index 0: not well-formed (invalid token)\u001b[0m\n",
      "\u001b[0m\u001b[31m2024-12-13 18:16:01.586 (71346.790s) [           9963E]       vtkXMLReader.cxx:576    ERR| vtkXMLImageDataReader (0x1166f2da0): Error parsing input file.  ReadXMLInformation aborting.\u001b[0m\n",
      "\u001b[0m\u001b[31m2024-12-13 18:16:01.586 (71346.790s) [           9963E]       vtkExecutive.cxx:730    ERR| vtkCompositeDataPipeline (0x112c6f050): Algorithm vtkXMLImageDataReader (0x1166f2da0) returned failure for request: vtkInformation (0x111220600)\n",
      "  Debug: Off\n",
      "  Modified Time: 286\n",
      "  Reference Count: 1\n",
      "  Registered Events: (none)\n",
      "  Request: REQUEST_INFORMATION\n",
      "  ALGORITHM_AFTER_FORWARD: 1\n",
      "  FORWARD_DIRECTION: 0\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageData' object has no attribute 'GetDataType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vtifile \u001b[38;5;241m=\u001b[39m read_vti(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.vti\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy_support\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvtk_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvtifile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should be (250, 250, 50)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Desktop/Autoencoder/.venv/lib/python3.11/site-packages/vtkmodules/util/numpy_support.py:215\u001b[0m, in \u001b[0;36mvtk_to_numpy\u001b[0;34m(vtk_array)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvtk_to_numpy\u001b[39m(vtk_array):\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a VTK data array to a numpy array.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Given a subclass of vtkDataArray, this function returns an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m \n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     typ \u001b[38;5;241m=\u001b[39m \u001b[43mvtk_array\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetDataType\u001b[49m()\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m typ \u001b[38;5;129;01min\u001b[39;00m get_vtk_to_numpy_typemap()\u001b[38;5;241m.\u001b[39mkeys(), \\\n\u001b[1;32m    217\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported array type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mtyp\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m typ \u001b[38;5;241m!=\u001b[39m vtkConstants\u001b[38;5;241m.\u001b[39mVTK_BIT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBit arrays are not supported.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageData' object has no attribute 'GetDataType'"
     ]
    }
   ],
   "source": [
    "vtifile = read_vti('test.vti')\n",
    "data = numpy_support.vtk_to_numpy(vtifile)\n",
    "print(data.shape)  # Should be (250, 250, 50)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_tensor, input_tensor) \n",
    "loader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 1, 250, 250, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv3DAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3DAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, stride=2, padding=1),  # Output: (16, 125, 125, 25)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1), # Output: (32, 63, 63, 13)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1), # Output: (64, 32, 32, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1), # Output: (128, 16, 16, 4)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 128 * 16 * 16 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 16, 16, 4)),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=(1,1,0)), # Match dimension to 32, 32, 8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=0), # Match dimension to 63, 63, 15\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=0), # Match dimension to 126, 126, 30\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1)), # Match dimension to 250, 250, 59\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = Conv3DAutoencoder()\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.rand(1, 1, 250, 250, 50)\n",
    "\n",
    "# Forward pass the dummy input through the model\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conv3DAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50  # Set the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, _ in loader:\n",
    "        data = data.to(device)\n",
    "        latent = model.encoder(data)\n",
    "        print(\"Latent Shape:\", latent.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlatent\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent' is not defined"
     ]
    }
   ],
   "source": [
    "print(latent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
