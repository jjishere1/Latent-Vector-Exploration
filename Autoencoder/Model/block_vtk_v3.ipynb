{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os \n",
    "from vtk import *\n",
    "from vtk.util import numpy_support\n",
    "from vtk import vtkXMLImageDataReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PSNR(arrgt,arr_recon):\n",
    "    try:\n",
    "        diff = arrgt - arr_recon\n",
    "        sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n",
    "        if(np.mean(diff**2) == 0):\n",
    "            raise ZeroDivisionError(\"dividing by zero, cannot calculate psnr\")\n",
    "        snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n",
    "        return snr\n",
    "    except ZeroDivisionError as err:\n",
    "        return str(err)\n",
    "\n",
    "def get_numpy_array_from_vtk_image_data(vtk_image_data):\n",
    "    point_data = vtk_image_data.GetPointData()\n",
    "    array = point_data.GetArray(0)\n",
    "    if array is None:\n",
    "        raise ValueError(\"No array found in vtkImageData.\")\n",
    "    \n",
    "    numpy_array = numpy_support.vtk_to_numpy(array)\n",
    "    dims = vtk_image_data.GetDimensions()  # Gets the dimensions of the vtkImageData\n",
    "    numpy_array = numpy_array.reshape(dims[1], dims[0], dims[2]) # Reshape according to VTK's dimension order\n",
    "    return numpy_array\n",
    "\n",
    "\n",
    "def read_vti(filename):\n",
    "    reader = vtkXMLImageDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "\n",
    "\n",
    "def writeVti(data, filename):\n",
    "    writer = vtkXMLImageDataWriter()\n",
    "    writer.SetFileName(filename)\n",
    "    writer.SetInputData(data)\n",
    "    writer.Write()\n",
    "\n",
    "\n",
    "def createVtkImageData(origin, dimensions, spacing):\n",
    "    localDataset = vtkImageData()\n",
    "    localDataset.SetOrigin(origin)\n",
    "    localDataset.SetDimensions(dimensions)\n",
    "    localDataset.SetSpacing(spacing)\n",
    "    return localDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 50)\n"
     ]
    }
   ],
   "source": [
    "vtifile = read_vti('Dataset/Pf25.binLE.raw_corrected_2_subsampled.vti')\n",
    "try:\n",
    "    data = get_numpy_array_from_vtk_image_data(vtifile)\n",
    "    print(data.shape)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "data = 2 * ((data - data.min()) / (data.max() - data.min())) - 1 \n",
    "four_d_tensor = torch.from_numpy(data).float() \n",
    "input_tensor = four_d_tensor.unsqueeze(0)\n",
    "\n",
    "block_dims = (25, 25, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 250, 50])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3DAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Flatten(start_dim=1, end_dim=-1)\n",
       "    (10): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=512, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Unflatten(dim=1, unflattened_size=(64, 4, 4, 1))\n",
       "    (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(0, 0, 1))\n",
       "    (8): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): ConvTranspose3d(32, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (11): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): ConvTranspose3d(16, 1, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (14): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv3DAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3DAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, stride=2, padding=1),  # Output: (16, 13, 13, 3)\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1),  # Output: (32, 7, 7, 2)\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: (64, 4, 4, 1)\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4 * 1, 512),  # Adjust for flattened dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32),  # Latent vector size\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64 * 4 * 4 * 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 4, 4, 1)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(0, 0, 1)),  # Output: (32, 7, 7, 2)\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=(0, 0, 0)),  # Output: (16, 13, 13, 5)\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=(0, 0, 0)),  # Output: (1, 25, 25, 5)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = Conv3DAutoencoder()\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy tensor with input size (1, 1, 25, 25, 5)\n",
    "# dummy_input = torch.randn(1, 1, 25, 25, 5)\n",
    "\n",
    "# # Pass the dummy tensor through the model\n",
    "# output = model(dummy_input)\n",
    "\n",
    "# # Print the output shape to verify\n",
    "# print(output.shape)  # Expected output: (1, 1, 25, 25, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_blocks(data, block_dims): # make blocks 5x5x5 \n",
    "\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "\n",
    "    blocks = []\n",
    "    height_step, width_step, depth_step = block_dims\n",
    "\n",
    "    for h in range(0, data.shape[1], height_step): \n",
    "        for w in range(0, data.shape[2], width_step):  \n",
    "            for d in range(0, data.shape[3], depth_step): \n",
    "                block = data[:, h:h + height_step, w:w + width_step, d:d + depth_step]\n",
    "                blocks.append(block)\n",
    "\n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conv3DAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BlockDataset(Dataset):\n",
    "    def __init__(self, blocks):\n",
    "        self.blocks = [torch.tensor(block, dtype=torch.float) for block in blocks]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.blocks[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_31615/4234565526.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.blocks = [torch.tensor(block, dtype=torch.float) for block in blocks]\n"
     ]
    }
   ],
   "source": [
    "blocks = divide_into_blocks(input_tensor, block_dims)\n",
    "dataset = BlockDataset(blocks)\n",
    "\n",
    "# Define DataLoader with a batch size\n",
    "batch_size = 16  # You can adjust this as needed\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0286\n",
      "Epoch 2, Loss: 0.0070\n",
      "Epoch 3, Loss: 0.0054\n",
      "Epoch 4, Loss: 0.0046\n",
      "Epoch 5, Loss: 0.0039\n",
      "Epoch 6, Loss: 0.0029\n",
      "Epoch 7, Loss: 0.0027\n",
      "Epoch 8, Loss: 0.0024\n",
      "Epoch 9, Loss: 0.0021\n",
      "Epoch 10, Loss: 0.0020\n",
      "Epoch 11, Loss: 0.0017\n",
      "Epoch 12, Loss: 0.0016\n",
      "Epoch 13, Loss: 0.0014\n",
      "Epoch 14, Loss: 0.0017\n",
      "Epoch 15, Loss: 0.0014\n",
      "Epoch 16, Loss: 0.0012\n",
      "Epoch 17, Loss: 0.0011\n",
      "Epoch 18, Loss: 0.0010\n",
      "Epoch 19, Loss: 0.0010\n",
      "Epoch 20, Loss: 0.0011\n",
      "Epoch 21, Loss: 0.0010\n",
      "Epoch 22, Loss: 0.0009\n",
      "Epoch 23, Loss: 0.0008\n",
      "Epoch 24, Loss: 0.0007\n",
      "Epoch 25, Loss: 0.0007\n",
      "Epoch 26, Loss: 0.0008\n",
      "Epoch 27, Loss: 0.0006\n",
      "Epoch 28, Loss: 0.0006\n",
      "Epoch 29, Loss: 0.0006\n",
      "Epoch 30, Loss: 0.0006\n",
      "Epoch 31, Loss: 0.0006\n",
      "Epoch 32, Loss: 0.0007\n",
      "Epoch 33, Loss: 0.0006\n",
      "Epoch 34, Loss: 0.0007\n",
      "Epoch 35, Loss: 0.0007\n",
      "Epoch 36, Loss: 0.0006\n",
      "Epoch 37, Loss: 0.0006\n",
      "Epoch 38, Loss: 0.0005\n",
      "Epoch 39, Loss: 0.0005\n",
      "Epoch 40, Loss: 0.0006\n",
      "Epoch 41, Loss: 0.0006\n",
      "Epoch 42, Loss: 0.0005\n",
      "Epoch 43, Loss: 0.0006\n",
      "Epoch 44, Loss: 0.0005\n",
      "Epoch 45, Loss: 0.0005\n",
      "Epoch 46, Loss: 0.0005\n",
      "Epoch 47, Loss: 0.0004\n",
      "Epoch 48, Loss: 0.0004\n",
      "Epoch 49, Loss: 0.0004\n",
      "Epoch 50, Loss: 0.0005\n",
      "Epoch 51, Loss: 0.0006\n",
      "Epoch 52, Loss: 0.0005\n",
      "Epoch 53, Loss: 0.0005\n",
      "Epoch 54, Loss: 0.0004\n",
      "Epoch 55, Loss: 0.0004\n",
      "Epoch 56, Loss: 0.0005\n",
      "Epoch 57, Loss: 0.0004\n",
      "Epoch 58, Loss: 0.0003\n",
      "Epoch 59, Loss: 0.0004\n",
      "Epoch 60, Loss: 0.0004\n",
      "Epoch 61, Loss: 0.0004\n",
      "Epoch 62, Loss: 0.0004\n",
      "Epoch 63, Loss: 0.0004\n",
      "Epoch 64, Loss: 0.0004\n",
      "Epoch 65, Loss: 0.0004\n",
      "Epoch 66, Loss: 0.0004\n",
      "Epoch 67, Loss: 0.0004\n",
      "Epoch 68, Loss: 0.0004\n",
      "Epoch 69, Loss: 0.0004\n",
      "Epoch 70, Loss: 0.0004\n",
      "Epoch 71, Loss: 0.0003\n",
      "Epoch 72, Loss: 0.0003\n",
      "Epoch 73, Loss: 0.0003\n",
      "Epoch 74, Loss: 0.0004\n",
      "Epoch 75, Loss: 0.0006\n",
      "Epoch 76, Loss: 0.0005\n",
      "Epoch 77, Loss: 0.0005\n",
      "Epoch 78, Loss: 0.0003\n",
      "Epoch 79, Loss: 0.0003\n",
      "Epoch 80, Loss: 0.0003\n",
      "Epoch 81, Loss: 0.0008\n",
      "Epoch 82, Loss: 0.0007\n",
      "Epoch 83, Loss: 0.0004\n",
      "Epoch 84, Loss: 0.0003\n",
      "Epoch 85, Loss: 0.0003\n",
      "Epoch 86, Loss: 0.0003\n",
      "Epoch 87, Loss: 0.0004\n",
      "Epoch 88, Loss: 0.0003\n",
      "Epoch 89, Loss: 0.0003\n",
      "Epoch 90, Loss: 0.0003\n",
      "Epoch 91, Loss: 0.0003\n",
      "Epoch 92, Loss: 0.0003\n",
      "Epoch 93, Loss: 0.0003\n",
      "Epoch 94, Loss: 0.0003\n",
      "Epoch 95, Loss: 0.0003\n",
      "Epoch 96, Loss: 0.0004\n",
      "Epoch 97, Loss: 0.0003\n",
      "Epoch 98, Loss: 0.0004\n",
      "Epoch 99, Loss: 0.0004\n",
      "Epoch 100, Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)  # Move batch to device (e.g., GPU)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.size(0)  # Accumulate total loss for the epoch\n",
    "    \n",
    "    avg_loss = total_loss / len(dataset)  # Compute average loss\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_31615/620529787.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "reconstructed_blocks = []\n",
    "with torch.no_grad():\n",
    "    for block in blocks:\n",
    "        block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        output = model(block_tensor).cpu().numpy()\n",
    "        reconstructed_blocks.append(output.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def reassemble_blocks(blocks, original_shape, block_dims):\n",
    "    reassembled_data = np.zeros(original_shape)\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for h in range(0, original_shape[1], block_dims[0]):  # Height\n",
    "        for w in range(0, original_shape[2], block_dims[1]):  # Width\n",
    "            for d in range(0, original_shape[3], block_dims[2]):  # Depth\n",
    "                # Ensuring the block is inserted into the correct slice\n",
    "                if isinstance(blocks[index], torch.Tensor):\n",
    "                    block_data = blocks[index].numpy()  # Convert to numpy if it's a tensor\n",
    "                else:\n",
    "                    block_data = blocks[index]\n",
    "                \n",
    "                reassembled_data[:, h:h + block_dims[0], w:w + block_dims[1], d:d + block_dims[2]] = block_data\n",
    "                index += 1\n",
    "\n",
    "    return reassembled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MSE Loss on Entire Data: 0.0002624118351377547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_31615/2779997330.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  original_tensor = torch.tensor(input_tensor, dtype=torch.float).to(device)\n"
     ]
    }
   ],
   "source": [
    "reassembled_data = reassemble_blocks(reconstructed_blocks, input_tensor.shape, block_dims)\n",
    "\n",
    "original_tensor = torch.tensor(input_tensor, dtype=torch.float).to(device)\n",
    "reassembled_tensor = torch.tensor(reassembled_data, dtype=torch.float).to(device)\n",
    "\n",
    "final_loss = criterion(reassembled_tensor,input_tensor)\n",
    "print(f'Final MSE Loss on Entire Data: {final_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 250, 250, 50)\n",
      "41.830765666748675\n"
     ]
    }
   ],
   "source": [
    "print(reassembled_data.shape)\n",
    "reconstructed_data = reassembled_data[0, :, :, :]\n",
    "\n",
    "psnr_score = compute_PSNR(data,reconstructed_data)\n",
    "print(psnr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data_array = reconstructed_data.flatten()\n",
    "vtk_array = numpy_support.numpy_to_vtk(flat_data_array)\n",
    "\n",
    "\n",
    "data = read_vti('Dataset/Pf25.binLE.raw_corrected_2_subsampled.vti')\n",
    "array = data.GetPointData().GetArray(0)\n",
    "\n",
    "dim = data.GetDimensions()\n",
    "spacing = data.GetSpacing()\n",
    "origin = data.GetOrigin()\n",
    "\n",
    "\n",
    "new_data = createVtkImageData(origin, dim, spacing)\n",
    "new_data.GetPointData().AddArray(vtk_array)\n",
    "\n",
    "writeVti(new_data, 'out3.vti')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
