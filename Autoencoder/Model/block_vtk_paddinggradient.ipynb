{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os \n",
    "import vtk\n",
    "from vtk import *\n",
    "from vtk.util import numpy_support\n",
    "from vtk import vtkXMLImageDataReader\n",
    "from vtk import vtkImageGradient, vtkImageMagnitude\n",
    "from vtkmodules.vtkCommonDataModel import vtkImageData\n",
    "from vtkmodules.util import numpy_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PSNR(arrgt,arr_recon):\n",
    "    try:\n",
    "        diff = arrgt - arr_recon\n",
    "        sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n",
    "        if(np.mean(diff**2) == 0):\n",
    "            raise ZeroDivisionError(\"dividing by zero, cannot calculate psnr\")\n",
    "        snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n",
    "        return snr\n",
    "    except ZeroDivisionError as err:\n",
    "        return str(err)\n",
    "\n",
    "def read_vti(filename):\n",
    "    reader = vtkXMLImageDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "\n",
    "\n",
    "def writeVti(data, filename):\n",
    "    writer = vtkXMLImageDataWriter()\n",
    "    writer.SetFileName(filename)\n",
    "    writer.SetInputData(data)\n",
    "    writer.Write()\n",
    "\n",
    "\n",
    "def createVtkImageData(origin, dimensions, spacing):\n",
    "    localDataset = vtkImageData()\n",
    "    localDataset.SetOrigin(origin)\n",
    "    localDataset.SetDimensions(dimensions)\n",
    "    localDataset.SetSpacing(spacing)\n",
    "    return localDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_magnitude(vtk_image_data):\n",
    "    if not isinstance(vtk_image_data, vtk.vtkImageData):\n",
    "        raise TypeError(\"compute_gradient_magnitude requires a vtkImageData object\")\n",
    "    \n",
    "    if vtk_image_data.GetPointData().GetScalars() is None:\n",
    "        raise ValueError(\"VTK ImageData has no scalars set. Cannot compute gradients.\")\n",
    "\n",
    "    gradient_filter = vtk.vtkImageGradient()\n",
    "    gradient_filter.SetInputData(vtk_image_data)\n",
    "    gradient_filter.SetDimensionality(3)\n",
    "    gradient_filter.Update()\n",
    "\n",
    "    magnitude_filter = vtk.vtkImageMagnitude()\n",
    "    magnitude_filter.SetInputConnection(gradient_filter.GetOutputPort())\n",
    "    magnitude_filter.Update()\n",
    "\n",
    "    return magnitude_filter.GetOutput()\n",
    "\n",
    "def get_numpy_array_from_vtk_image_data(vtk_image_data):\n",
    "    point_data = vtk_image_data.GetPointData()\n",
    "    array = point_data.GetScalars()  \n",
    "    \n",
    "    if array is None:\n",
    "        raise ValueError(\"No scalar array found in vtkImageData.\")\n",
    "    \n",
    "    numpy_array = numpy_support.vtk_to_numpy(array)\n",
    "    dims = vtk_image_data.GetDimensions()  \n",
    "    num_components = array.GetNumberOfComponents()\n",
    "\n",
    "    expected_size = dims[0] * dims[1] * dims[2] * num_components\n",
    "\n",
    "    if numpy_array.size != expected_size:\n",
    "        raise ValueError(f\"Shape mismatch! Cannot reshape {numpy_array.size} elements into {dims[2], dims[1], dims[0], num_components}\")\n",
    "\n",
    "    numpy_array = numpy_array.reshape(dims[2], dims[1], dims[0], num_components)  \n",
    "    return numpy_array.squeeze()  \n",
    "\n",
    "def tensor_to_vtk_image_data(tensor):\n",
    "    np_array = tensor.detach().cpu().numpy()\n",
    "\n",
    "    if np_array.ndim == 4:  \n",
    "        np_array = np_array[0] \n",
    "\n",
    "    vtk_image_data = vtk.vtkImageData()\n",
    "\n",
    "    dims = np_array.shape  \n",
    "    vtk_image_data.SetDimensions(dims[2], dims[1], dims[0])  \n",
    "\n",
    "    flat_array = np_array.flatten(order='F')\n",
    "    vtk_array = numpy_support.numpy_to_vtk(flat_array, deep=True, array_type=vtk.VTK_FLOAT)\n",
    "    vtk_image_data.GetPointData().SetScalars(vtk_array)\n",
    "    \n",
    "    return vtk_image_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 250, 250)\n"
     ]
    }
   ],
   "source": [
    "vtifile = read_vti('/Users/manasvijain/Desktop/Autoencoder/Dataset/Pf25.binLE.raw_corrected_2_subsampled.vti')\n",
    "try:\n",
    "    data = get_numpy_array_from_vtk_image_data(vtifile)\n",
    "    print(data.shape)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_magnitude = compute_gradient_magnitude(vtifile)\n",
    "gradient_magnitude_array = get_numpy_array_from_vtk_image_data(gradient_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "data = 2 * ((data - data.min()) / (data.max() - data.min())) - 1 \n",
    "four_d_tensor = torch.from_numpy(data).float() \n",
    "input_tensor = four_d_tensor.unsqueeze(0)\n",
    "\n",
    "block_dims = (5, 5, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 250, 250])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3DAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Flatten(start_dim=1, end_dim=-1)\n",
       "    (10): Linear(in_features=46656, out_features=512, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=512, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=46656, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Unflatten(dim=1, unflattened_size=(64, 9, 9, 9))\n",
       "    (5): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (8): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): ConvTranspose3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (11): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): ConvTranspose3d(16, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (14): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv3DAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3DAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(64 * 5 * 5 * 5, 512),\n",
    "            nn.Linear(64 * 9 * 9 * 9, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32,512),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(512, 64 * 5 * 5 * 5),\n",
    "            nn.Linear(512, 64 * 9 * 9 * 9),\n",
    "            nn.ReLU(),\n",
    "            # nn.Unflatten(1, (64, 5, 5, 5)),\n",
    "            nn.Unflatten(1, (64, 9, 9, 9)),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(16), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, 1, kernel_size=3, stride=1, padding=1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = Conv3DAutoencoder()\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_blocks(data, block_dims): # make blocks 5x5x5 \n",
    "\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "\n",
    "    blocks = []\n",
    "    height_step, width_step, depth_step = block_dims\n",
    "\n",
    "    for h in range(0, data.shape[1], height_step): \n",
    "        for w in range(0, data.shape[2], width_step):  \n",
    "            for d in range(0, data.shape[3], depth_step): \n",
    "                block = data[:, h:h + height_step, w:w + width_step, d:d + depth_step]\n",
    "                blocks.append(block)\n",
    "\n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Conv3DAutoencoder().to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conv3DAutoencoder().to(device)\n",
    "mse_criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def custom_loss(output, target, gradient_target, alpha=0.5):\n",
    "\n",
    "    reconstruction_loss = mse_criterion(output, target)\n",
    "    \n",
    "    output_vtk = tensor_to_vtk_image_data(output)\n",
    "    output_gradient_magnitude = compute_gradient_magnitude(output_vtk)\n",
    "    output_gradient_magnitude_array = get_numpy_array_from_vtk_image_data(output_gradient_magnitude)\n",
    "\n",
    "\n",
    "    assert output_gradient_magnitude_array.shape == gradient_target.shape, \"Shapes must match\"\n",
    "    output_magnitude = np.linalg.norm(output_gradient_magnitude_array)\n",
    "    gradient_target_magnitude = np.linalg.norm(gradient_target)\n",
    "    gradient_loss = np.abs(output_magnitude - gradient_target_magnitude)\n",
    "\n",
    "    total_loss = alpha * reconstruction_loss + (1 - alpha) * gradient_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BlockDataset(Dataset):\n",
    "    def __init__(self, blocks):\n",
    "        self.blocks = [torch.tensor(block, dtype=torch.float) for block in blocks]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.blocks[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_blocks(data, block_shape, padded_shape, pad_value=0):\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "    if data.ndim == 3:\n",
    "        data = data.unsqueeze(0)\n",
    "    pad_h = padded_shape[0] // 2\n",
    "    pad_w = padded_shape[1] // 2\n",
    "    pad_d = padded_shape[2] // 2\n",
    "    padded_data = F.pad(data, (pad_d, pad_d, pad_w, pad_w, pad_h, pad_h), mode='constant', value=pad_value)\n",
    "    _, H, W, D = data.shape\n",
    "    n_blocks_h = H // block_shape[0]\n",
    "    n_blocks_w = W // block_shape[1]\n",
    "    n_blocks_d = D // block_shape[2]\n",
    "    blocks_list = []\n",
    "    for i in range(n_blocks_h):\n",
    "        for j in range(n_blocks_w):\n",
    "            for k in range(n_blocks_d):\n",
    "                center_h = i * block_shape[0] + (block_shape[0] // 2)\n",
    "                center_w = j * block_shape[1] + (block_shape[1] // 2)\n",
    "                center_d = k * block_shape[2] + (block_shape[2] // 2)\n",
    "                c_h = center_h + pad_h\n",
    "                c_w = center_w + pad_w\n",
    "                c_d = center_d + pad_d\n",
    "                block = padded_data[:, \n",
    "                    c_h - pad_h : c_h + pad_h + 1,\n",
    "                    c_w - pad_w : c_w + pad_w + 1,\n",
    "                    c_d - pad_d : c_d + pad_d + 1\n",
    "                ]\n",
    "                blocks_list.append(block)\n",
    "    return blocks_list\n",
    "\n",
    "def unpad_block(padded_block, block_dims, padded_shape):\n",
    "    start_h = padded_shape[0] // 2 - block_dims[0] // 2\n",
    "    start_w = padded_shape[1] // 2 - block_dims[1] // 2\n",
    "    start_d = padded_shape[2] // 2 - block_dims[2] // 2\n",
    "    return padded_block[:, start_h:start_h + block_dims[0],\n",
    "                           start_w:start_w + block_dims[1],\n",
    "                           start_d:start_d + block_dims[2]]\n",
    "\n",
    "def reassemble_blocks(blocks, original_shape, block_dims, padded_shape):\n",
    "    reassembled_data = np.zeros(original_shape)\n",
    "    index = 0\n",
    "    for h in range(0, original_shape[1], block_dims[0]):\n",
    "        for w in range(0, original_shape[2], block_dims[1]):\n",
    "            for d in range(0, original_shape[3], block_dims[2]):\n",
    "                block = blocks[index]\n",
    "                unpadded = unpad_block(block, block_dims, padded_shape)\n",
    "                block_data = unpadded.numpy()\n",
    "                reassembled_data[:, h:h + block_dims[0],\n",
    "                                  w:w + block_dims[1],\n",
    "                                  d:d + block_dims[2]] = block_data\n",
    "                index += 1\n",
    "    return reassembled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_53850/4234565526.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.blocks = [torch.tensor(block, dtype=torch.float) for block in blocks]\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.rand(1, 250, 250, 50)\n",
    "block_dims = (5, 5, 5)\n",
    "padded_shape = (9, 9, 9)\n",
    "pad_value = 0\n",
    "\n",
    "blocks = pad_blocks(input_tensor, block_dims, padded_shape, pad_value)\n",
    "dataset = BlockDataset(blocks)\n",
    "batch_size = 16  \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0814\n",
      "Epoch 2, Loss: 1.0087\n",
      "Epoch 3, Loss: 0.9950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss(output, batch, batch_gradient_magnitude_array, alpha)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \n",
      "File \u001b[0;32m~/Desktop/Autoencoder/.venv/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Autoencoder/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Autoencoder/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "# num_epochs = 50\n",
    "num_epochs = 10\n",
    "alpha = 0.5  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)  \n",
    "        \n",
    "        batch_vtk = tensor_to_vtk_image_data(batch)\n",
    "        batch_gradient_magnitude = compute_gradient_magnitude(batch_vtk)\n",
    "        batch_gradient_magnitude_array = get_numpy_array_from_vtk_image_data(batch_gradient_magnitude)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        \n",
    "        loss = custom_loss(output, batch, batch_gradient_magnitude_array, alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.size(0)  \n",
    "    \n",
    "    avg_loss = total_loss / len(dataset) \n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_13744/620529787.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "reconstructed_blocks = []\n",
    "with torch.no_grad():\n",
    "    for block in blocks:\n",
    "        block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        output = model(block_tensor).cpu().numpy()\n",
    "        reconstructed_blocks.append(output.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassemble_blocks(blocks, original_shape, block_dims):\n",
    "    reassembled_data = np.zeros(original_shape)\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for h in range(0, original_shape[1], block_dims[0]):  # Height\n",
    "        for w in range(0, original_shape[2], block_dims[1]):  # Width\n",
    "            for d in range(0, original_shape[3], block_dims[2]):  # Depth\n",
    "                # Ensuring the block is inserted into the correct slice\n",
    "                if isinstance(blocks[index], torch.Tensor):\n",
    "                    block_data = blocks[index].numpy()  # Convert to numpy if it's a tensor\n",
    "                else:\n",
    "                    block_data = blocks[index]\n",
    "                \n",
    "                reassembled_data[:, h:h + block_dims[0], w:w + block_dims[1], d:d + block_dims[2]] = block_data\n",
    "                index += 1\n",
    "\n",
    "    return reassembled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3179831414.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    unpadded_blocks = unpad_blocks(reconstructed_blocks, block_dims, padded_shape)reassembled_data = reassemble_blocks(reconstructed_blocks, input_tensor.shape, block_dims)\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "unpadded_blocks = unpad_blocks(reconstructed_blocks, block_dims, padded_shape)\n",
    "reassembled_data = reassemble_blocks(unpadded_blocks, input_tensor.shape, block_dims)\n",
    "\n",
    "original_tensor = torch.tensor(input_tensor, dtype=torch.float).to(device)\n",
    "reassembled_tensor = torch.tensor(reassembled_data, dtype=torch.float).to(device)\n",
    "\n",
    "final_loss = mse_criterion(reassembled_tensor,input_tensor)\n",
    "print(f'Final MSE Loss on Entire Data: {final_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 250, 250)\n",
      "41.49894151745377\n"
     ]
    }
   ],
   "source": [
    "print(reassembled_data.shape)\n",
    "reconstructed_data = reassembled_data[0, :, :, :]\n",
    "\n",
    "psnr_score = compute_PSNR(data,reconstructed_data)\n",
    "print(psnr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data_array = reconstructed_data.flatten()\n",
    "vtk_array = numpy_support.numpy_to_vtk(flat_data_array)\n",
    "\n",
    "\n",
    "data = read_vti('/Users/manasvijain/Desktop/Autoencoder/Dataset/Pf25.binLE.raw_corrected_2_subsampled.vti')\n",
    "array = data.GetPointData().GetArray(0)\n",
    "\n",
    "dim = data.GetDimensions()\n",
    "spacing = data.GetSpacing()\n",
    "origin = data.GetOrigin()\n",
    "\n",
    "\n",
    "new_data = createVtkImageData(origin, dim, spacing)\n",
    "new_data.GetPointData().AddArray(vtk_array)\n",
    "\n",
    "writeVti(new_data, 'out100epochs.vti')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
