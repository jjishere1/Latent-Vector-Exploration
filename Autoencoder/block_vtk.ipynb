{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os \n",
    "from vtk import *\n",
    "from vtk.util import numpy_support\n",
    "from vtk import vtkXMLImageDataReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PSNR(arrgt,arr_recon):\n",
    "    try:\n",
    "        diff = arrgt - arr_recon\n",
    "        sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n",
    "        if(np.mean(diff**2) == 0):\n",
    "            raise ZeroDivisionError(\"dividing by zero, cannot calculate psnr\")\n",
    "        snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n",
    "        return snr\n",
    "    except ZeroDivisionError as err:\n",
    "        return str(err)\n",
    "\n",
    "def get_numpy_array_from_vtk_image_data(vtk_image_data):\n",
    "    point_data = vtk_image_data.GetPointData()\n",
    "    array = point_data.GetArray(0)\n",
    "    if array is None:\n",
    "        raise ValueError(\"No array found in vtkImageData.\")\n",
    "    \n",
    "    numpy_array = numpy_support.vtk_to_numpy(array)\n",
    "    dims = vtk_image_data.GetDimensions()  # Gets the dimensions of the vtkImageData\n",
    "    numpy_array = numpy_array.reshape(dims[1], dims[0], dims[2]) # Reshape according to VTK's dimension order\n",
    "    return numpy_array\n",
    "\n",
    "\n",
    "def read_vti(filename):\n",
    "    reader = vtkXMLImageDataReader()\n",
    "    reader.SetFileName(filename)\n",
    "    reader.Update()\n",
    "    return reader.GetOutput()\n",
    "\n",
    "\n",
    "def writeVti(data, filename):\n",
    "    writer = vtkXMLImageDataWriter()\n",
    "    writer.SetFileName(filename)\n",
    "    writer.SetInputData(data)\n",
    "    writer.Write()\n",
    "\n",
    "\n",
    "def createVtkImageData(origin, dimensions, spacing):\n",
    "    localDataset = vtkImageData()\n",
    "    localDataset.SetOrigin(origin)\n",
    "    localDataset.SetDimensions(dimensions)\n",
    "    localDataset.SetSpacing(spacing)\n",
    "    return localDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 50)\n"
     ]
    }
   ],
   "source": [
    "vtifile = read_vti('Dataset/Pf25.binLE.raw_corrected_2_subsampled.vti')\n",
    "try:\n",
    "    data = get_numpy_array_from_vtk_image_data(vtifile)\n",
    "    print(data.shape)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "# data = (data - data.min()) / (data.max() - data.min()) # do it -1 to 1 \n",
    "data = 2 * ((data - data.min()) / (data.max() - data.min())) - 1 \n",
    "#try standardscalar as well \n",
    "four_d_tensor = torch.from_numpy(data).float() \n",
    "input_tensor = four_d_tensor.unsqueeze(0)\n",
    "\n",
    "block_dims = (125, 125, 25) \n",
    "# block_dims = (5, 5, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_data_array = data.flatten()\n",
    "\n",
    "# # Convert numpy array to VTK array\n",
    "# vtk_array = numpy_support.numpy_to_vtk(flat_data_array)\n",
    "# vtk_array.SetName(\"NormalizedData\")  # Optional: set the array name for identification in ParaView\n",
    "\n",
    "\n",
    "\n",
    "# rawdata = read_vti('Dataset/Pf25.binLE.raw_corrected_2_subsampled.vti')\n",
    "# array = rawdata.GetPointData().GetArray(0)\n",
    "\n",
    "# dim = rawdata.GetDimensions()\n",
    "# spacing = rawdata.GetSpacing()\n",
    "# origin = rawdata.GetOrigin()\n",
    "\n",
    "# # Get the dimensions, spacing, and origin from the original VTI file\n",
    "# # dim = vtifile.GetDimensions()\n",
    "# # spacing = vtifile.GetSpacing()\n",
    "# # origin = vtifile.GetOrigin()\n",
    "\n",
    "# # Create a new VTK ImageData object\n",
    "# new_data = createVtkImageData(origin, dim, spacing)\n",
    "\n",
    "# # Add the VTK array to the ImageData object\n",
    "# new_data.GetPointData().SetScalars(vtk_array)\n",
    "\n",
    "# # Write to a VTI file\n",
    "# writeVti(new_data, 'normalized_data.vti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250, 250, 50])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3DAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (10): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "    (13): Linear(in_features=16384, out_features=1024, bias=True)\n",
       "    (14): ReLU()\n",
       "    (15): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=16384, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Unflatten(dim=1, unflattened_size=(128, 8, 8, 2))\n",
       "    (5): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "    (8): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 0))\n",
       "    (11): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): ConvTranspose3d(32, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (14): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): ReLU()\n",
       "    (16): ConvTranspose3d(16, 1, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (17): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv3DAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv3DAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, stride=2, padding=1),  # Output: (16, 63, 63, 13)\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, stride=2, padding=1), # Output: (32, 32, 32, 7)\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1), # Output: (64, 16, 16, 4)\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1, padding_mode='zeros'), # Output: (128, 8, 8, 2)\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8 * 2, 1024),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            # nn.ReLU() #dont do this \n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.8), #remove dropout\n",
    "            nn.Linear(1024, 128 * 8 * 8 * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 8, 8, 2)),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(1,1,0)), \n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=0),\n",
    "            nn.BatchNorm3d(16), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=0), \n",
    "            # nn.Sigmoid() #use tanh\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = Conv3DAutoencoder()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_blocks(data, block_dims): # make blocks 5x5x5 \n",
    "\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "\n",
    "    blocks = []\n",
    "    height_step, width_step, depth_step = block_dims\n",
    "\n",
    "    for h in range(0, data.shape[1], height_step): \n",
    "        for w in range(0, data.shape[2], width_step):  \n",
    "            for d in range(0, data.shape[3], depth_step): \n",
    "                block = data[:, h:h + height_step, w:w + width_step, d:d + depth_step]\n",
    "                blocks.append(block)\n",
    "\n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conv3DAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_22140/1244637609.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3905640672892332\n",
      "Epoch 2, Loss: 0.2491377331316471\n",
      "Epoch 3, Loss: 0.18733981810510159\n",
      "Epoch 4, Loss: 0.14240751694887877\n",
      "Epoch 5, Loss: 0.10894173244014382\n",
      "Epoch 6, Loss: 0.08429577434435487\n",
      "Epoch 7, Loss: 0.06605103984475136\n",
      "Epoch 8, Loss: 0.053325130604207516\n",
      "Epoch 9, Loss: 0.04430016363039613\n",
      "Epoch 10, Loss: 0.03791127190925181\n",
      "Epoch 11, Loss: 0.034095620503649116\n",
      "Epoch 12, Loss: 0.031217836309224367\n",
      "Epoch 13, Loss: 0.029366314760409296\n",
      "Epoch 14, Loss: 0.027644895599223673\n",
      "Epoch 15, Loss: 0.02648532041348517\n",
      "Epoch 16, Loss: 0.025625170208513737\n",
      "Epoch 17, Loss: 0.024818112491630018\n",
      "Epoch 18, Loss: 0.023944206070154905\n",
      "Epoch 19, Loss: 0.02331663272343576\n",
      "Epoch 20, Loss: 0.022724182228557765\n",
      "Epoch 21, Loss: 0.022300655720755458\n",
      "Epoch 22, Loss: 0.022583245998248458\n",
      "Epoch 23, Loss: 0.022140466258861125\n",
      "Epoch 24, Loss: 0.021798150381073356\n",
      "Epoch 25, Loss: 0.020468764239922166\n",
      "Epoch 26, Loss: 0.020887388673145324\n",
      "Epoch 27, Loss: 0.021873077377676964\n",
      "Epoch 28, Loss: 0.01820435974514112\n",
      "Epoch 29, Loss: 0.018362443253863603\n",
      "Epoch 30, Loss: 0.01749828871106729\n",
      "Epoch 31, Loss: 0.01688072393881157\n",
      "Epoch 32, Loss: 0.016437450191006064\n",
      "Epoch 33, Loss: 0.01601701753679663\n",
      "Epoch 34, Loss: 0.01562434429069981\n",
      "Epoch 35, Loss: 0.015229293407173827\n",
      "Epoch 36, Loss: 0.014840543328318745\n",
      "Epoch 37, Loss: 0.014441778970649466\n",
      "Epoch 38, Loss: 0.014027485536644235\n",
      "Epoch 39, Loss: 0.013633486873004586\n",
      "Epoch 40, Loss: 0.013204941875301301\n",
      "Epoch 41, Loss: 0.012845464516431093\n",
      "Epoch 42, Loss: 0.012387908936943859\n",
      "Epoch 43, Loss: 0.012029460864141583\n",
      "Epoch 44, Loss: 0.011500914377393201\n",
      "Epoch 45, Loss: 0.011120053444756195\n",
      "Epoch 46, Loss: 0.010774365946417674\n",
      "Epoch 47, Loss: 0.01060260142548941\n",
      "Epoch 48, Loss: 0.010648297029547393\n",
      "Epoch 49, Loss: 0.010797399299917743\n",
      "Epoch 50, Loss: 0.011569913185667247\n"
     ]
    }
   ],
   "source": [
    "blocks = divide_into_blocks(input_tensor, block_dims)\n",
    "model.train() \n",
    "#channel 32,5,5,5 4d tensor \n",
    "num_epochs = 50 \n",
    "#pytorch dataset #batch size add\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for block in blocks:\n",
    "        block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(block_tensor)\n",
    "        loss = criterion(output, block_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(blocks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_22140/620529787.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "reconstructed_blocks = []\n",
    "with torch.no_grad():\n",
    "    for block in blocks:\n",
    "        block_tensor = torch.tensor(block, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        output = model(block_tensor).cpu().numpy()\n",
    "        reconstructed_blocks.append(output.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def reassemble_blocks(blocks, original_shape, block_dims):\n",
    "    reassembled_data = np.zeros(original_shape)\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for h in range(0, original_shape[1], block_dims[0]):  # Height\n",
    "        for w in range(0, original_shape[2], block_dims[1]):  # Width\n",
    "            for d in range(0, original_shape[3], block_dims[2]):  # Depth\n",
    "                # Ensuring the block is inserted into the correct slice\n",
    "                if isinstance(blocks[index], torch.Tensor):\n",
    "                    block_data = blocks[index].numpy()  # Convert to numpy if it's a tensor\n",
    "                else:\n",
    "                    block_data = blocks[index]\n",
    "                \n",
    "                reassembled_data[:, h:h + block_dims[0], w:w + block_dims[1], d:d + block_dims[2]] = block_data\n",
    "                index += 1\n",
    "\n",
    "    return reassembled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MSE Loss on Entire Data: 0.02266939915716648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/bkfc3t293lv3g3bt0lmzwnp40000gn/T/ipykernel_22140/2779997330.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  original_tensor = torch.tensor(input_tensor, dtype=torch.float).to(device)\n"
     ]
    }
   ],
   "source": [
    "reassembled_data = reassemble_blocks(reconstructed_blocks, input_tensor.shape, block_dims)\n",
    "\n",
    "original_tensor = torch.tensor(input_tensor, dtype=torch.float).to(device)\n",
    "reassembled_tensor = torch.tensor(reassembled_data, dtype=torch.float).to(device)\n",
    "\n",
    "final_loss = criterion(reassembled_tensor,input_tensor)\n",
    "print(f'Final MSE Loss on Entire Data: {final_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 250, 250, 50)\n",
      "22.46619937273561\n"
     ]
    }
   ],
   "source": [
    "print(reassembled_data.shape)\n",
    "reconstructed_data = reassembled_data[0, :, :, :]\n",
    "\n",
    "psnr_score = compute_PSNR(data,reconstructed_data)\n",
    "print(psnr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data_array = reconstructed_data.flatten()\n",
    "vtk_array = numpy_support.numpy_to_vtk(flat_data_array)\n",
    "\n",
    "\n",
    "data = read_vti('Dataset/Pf25.binLE.raw_corrected_2_subsampled.vti')\n",
    "array = data.GetPointData().GetArray(0)\n",
    "\n",
    "dim = data.GetDimensions()\n",
    "spacing = data.GetSpacing()\n",
    "origin = data.GetOrigin()\n",
    "\n",
    "\n",
    "new_data = createVtkImageData(origin, dim, spacing)\n",
    "new_data.GetPointData().AddArray(vtk_array)\n",
    "\n",
    "writeVti(new_data, 'out.vti')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
